{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "agzCFUD0E9Bp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize for VGG\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoRyeDAhFbof",
        "outputId": "2504e73b-298e-48bd-ce6f-c4759bedf892"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained VGG16 model\n",
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "# Modify the classifier to match CIFAR-10 classes\n",
        "model.classifier[6] = nn.Linear(model.classifier[6].in_features, 10)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-WMKYwkFfpm",
        "outputId": "7978a8a0-891f-45d6-999e-0d9a89749e1c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "YyGrAPYiFrLt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(train_loader, 0):\n",
        "          inputs, labels = data  # Get the inputs and labels\n",
        "\n",
        "          # Move inputs and labels to the same device as the model\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()  # Zero the parameter gradients\n",
        "          outputs = model(inputs)  # Forward pass\n",
        "          loss = criterion(outputs, labels)  # Compute loss\n",
        "          loss.backward()  # Backward pass\n",
        "          optimizer.step()  # Optimize the weights\n",
        "\n",
        "          running_loss += loss.item()\n",
        "          if i % 100 == 99:  # Print every 100 mini-batches\n",
        "              print(f' Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
        "              running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acud3xM_GftN",
        "outputId": "754de997-f111-4452-f164-009387d6669d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Step [100/782], Loss: 2.2885\n",
            " Step [200/782], Loss: 2.2748\n",
            " Step [300/782], Loss: 1.9804\n",
            " Step [400/782], Loss: 1.6585\n",
            " Step [500/782], Loss: 1.3654\n",
            " Step [600/782], Loss: 1.0843\n",
            " Step [700/782], Loss: 0.8683\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy = evaluate_model(model, test_loader)\n",
        "print(f'Accuracy of the model on the CIFAR-10 test set: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZyKC6NvF0S9",
        "outputId": "1617ebc5-7105-4a4a-cdb9-38f064497479"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model on the CIFAR-10 test set: 78.53%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.applications import VGG16, VGG19, InceptionV3\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "EcDeRx_7R3QD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.0,  # Normalize pixel values to [0, 1]\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "XPi0LijDSO8h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the training and validation generators\n",
        "train_generator = datagen.flow(x_train, y_train, batch_size=32)  # Adjust batch size as needed\n",
        "validation_generator = datagen.flow(x_test, y_test, batch_size=32)\n",
        "\n",
        "# Initialize the VGG16 model\n",
        "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "\n",
        "\n",
        "# Train VGG19\n",
        "\n",
        "vgg19_model = VGG19(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n"
      ],
      "metadata": {
        "id": "yj--YfxWRl4W"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(base_model):\n",
        "    model = models.Sequential()\n",
        "    model.add(base_model)\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(10, activation='softmax'))  # 10 classes for CIFAR-10\n",
        "    return model\n",
        "\n",
        "# Create models\n",
        "vgg16_final_model = create_model(vgg16_model)\n",
        "vgg19_final_model = create_model(vgg19_model)"
      ],
      "metadata": {
        "id": "CQhaIeslRo3d"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile models\n",
        "vgg16_final_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "vgg19_final_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "McJJXQBYRqYo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train VGG16\n",
        "vgg16_history = vgg16_final_model.fit(datagen.flow(x_train, y_train, batch_size=64),\n",
        "                                       epochs=10,\n",
        "                                       validation_data=(x_test, y_test))\n",
        "\n",
        "# Train VGG19\n",
        "vgg19_history = vgg19_final_model.fit(datagen.flow(x_train, y_train, batch_size=64),\n",
        "                                       epochs=10,\n",
        "                                       validation_data=(x_test, y_test))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL0oXEPERss-",
        "outputId": "8eb00a2c-1460-4657-f67a-6798561606fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 74ms/step - accuracy: 0.1009 - loss: 2.3306 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 2/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 63ms/step - accuracy: 0.1013 - loss: 2.3027 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 3/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 63ms/step - accuracy: 0.0984 - loss: 2.3027 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 4/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 64ms/step - accuracy: 0.0990 - loss: 2.3027 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 5/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 62ms/step - accuracy: 0.0978 - loss: 2.3027 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 6/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 74ms/step - accuracy: 0.0943 - loss: 2.3027 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 7/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 73ms/step - accuracy: 0.0964 - loss: 2.3027 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 8/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 62ms/step - accuracy: 0.0969 - loss: 2.3027 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 9/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 65ms/step - accuracy: 0.0995 - loss: 2.3027 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 10/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 62ms/step - accuracy: 0.0981 - loss: 2.3027 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 1/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 76ms/step - accuracy: 0.0989 - loss: 2.3342 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 2/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 68ms/step - accuracy: 0.0958 - loss: 2.3027 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 3/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 71ms/step - accuracy: 0.0999 - loss: 2.3027 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 4/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 70ms/step - accuracy: 0.0990 - loss: 2.3027 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 5/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 69ms/step - accuracy: 0.0986 - loss: 2.3027 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 6/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 71ms/step - accuracy: 0.0945 - loss: 2.3027 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 7/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 69ms/step - accuracy: 0.1020 - loss: 2.3027 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 8/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 69ms/step - accuracy: 0.0970 - loss: 2.3027 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 9/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 67ms/step - accuracy: 0.0973 - loss: 2.3027 - val_accuracy: 0.1000 - val_loss: 2.3026\n",
            "Epoch 10/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 68ms/step - accuracy: 0.1003 - loss: 2.3026 - val_accuracy: 0.1000 - val_loss: 2.3026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get validation accuracies\n",
        "vgg16_val_acc = max(vgg16_history.history['val_accuracy'])\n",
        "vgg19_val_acc = max(vgg19_history.history['val_accuracy'])\n",
        "\n",
        "print(f'VGG16 Validation Accuracy: {vgg16_val_acc:.2f}')\n",
        "print(f'VGG19 Validation Accuracy: {vgg19_val_acc:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI165W3fRt5s",
        "outputId": "b4ed73ab-4c15-4a8a-90e1-e4b11ae80851"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG16 Validation Accuracy: 0.10\n",
            "VGG19 Validation Accuracy: 0.10\n"
          ]
        }
      ]
    }
  ]
}